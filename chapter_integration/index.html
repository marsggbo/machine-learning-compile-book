<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>5. 与机器学习框架的整合 &#8212; 机器学习编译 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="shortcut icon" href="../_static/mlc-favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. GPU 硬件加速" href="../chapter_gpu_acceleration/index.html" />
    <link rel="prev" title="4. 自动程序优化" href="../chapter_auto_program_optimization/index.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active"><span class="section-number">5. </span>与机器学习框架的整合</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_integration/index.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://mlc.ai/summer22-zh">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/mlc-ai/mlc-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://mlc.ai">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/mlc-logo-with-text-landscape.svg" alt="机器学习编译"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_tensor_program/index.html">2. 张量程序抽象</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensor_program.html">2.1. 元张量函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensor_program.html#id2">2.2. 张量程序抽象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensor_program.html#id4">2.3. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/case_study.html">2.4. TensorIR: 张量程序抽象案例研究</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensorir_exercises.html">2.5. TensorIR 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_end_to_end/index.html">3. 端到端模型执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_auto_program_optimization/index.html">4. 自动程序优化</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">5. 与机器学习框架的整合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gpu_acceleration/index.html">6. GPU 硬件加速</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gpu_acceleration/part1.html">6.1. 第一部分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gpu_acceleration/part2.html">6.2. 第二部分</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_graph_optimization/index.html">7. 计算图优化</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/mlc-logo-with-text-landscape.svg" alt="机器学习编译"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_tensor_program/index.html">2. 张量程序抽象</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensor_program.html">2.1. 元张量函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensor_program.html#id2">2.2. 张量程序抽象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensor_program.html#id4">2.3. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/case_study.html">2.4. TensorIR: 张量程序抽象案例研究</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_tensor_program/tensorir_exercises.html">2.5. TensorIR 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_end_to_end/index.html">3. 端到端模型执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_auto_program_optimization/index.html">4. 自动程序优化</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">5. 与机器学习框架的整合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gpu_acceleration/index.html">6. GPU 硬件加速</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gpu_acceleration/part1.html">6.1. 第一部分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gpu_acceleration/part2.html">6.2. 第二部分</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_graph_optimization/index.html">7. 计算图优化</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">5. </span>与机器学习框架的整合<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<div class="section" id="id2">
<h2><span class="section-number">5.1. </span>前言<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<p>在过去的章节中，我们学习了机器学习编译的抽象和张量函数之间的变换。</p>
<p>本章将讨论如何将机器学习模型从现有的机器学习框架引入 MLC 流程。</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">5.2. </span>准备工作<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h2>
<p>首先，我们导入必要的依赖项。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">relax</span>
<span class="kn">from</span> <span class="nn">tvm.ir.module</span> <span class="kn">import</span> <span class="n">IRModule</span>
<span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">relax</span> <span class="k">as</span> <span class="n">R</span>
<span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">tir</span> <span class="k">as</span> <span class="n">T</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">fx</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</pre></div>
</div>
</div>
<div class="section" id="builder-irmodule">
<h2><span class="section-number">5.3. </span>通过 Builder 构造 IRModule<a class="headerlink" href="#builder-irmodule" title="Permalink to this heading">¶</a></h2>
<p>在过去的章节中，我们一直在通过直接编写 TVMScript 来构建 IRModule。
随着模型变得越来越大，我们需要一种编程方式来构建
IRModule。在本节中，我们回顾一些支持该过程的工具。</p>
<div class="section" id="tensorir">
<h3><span class="section-number">5.3.1. </span>从张量表达式构造 TensorIR<a class="headerlink" href="#tensorir" title="Permalink to this heading">¶</a></h3>
<p>首先，我们回顾张量表达式 (tensor expression, TE) 这一领域特定语言来构建
TensorIR 函数。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
</pre></div>
</div>
<p>我们首先创建一个 placeholder，它表示 TensorIR 函数的输入。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>这里的每个输入和中间结果都表示为一个 <code class="docutils literal notranslate"><span class="pre">te.Tensor</span></code> 对象。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
<p>每个 <code class="docutils literal notranslate"><span class="pre">te.Tensor</span></code> 都有一个 shape 字段和 dtype 字段，用于记录计算的
shape 和数据类型。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<p>我们可以通过一系列张量表达式来描述计算。这里的 <code class="docutils literal notranslate"><span class="pre">te.compute</span></code> 使用
<code class="docutils literal notranslate"><span class="pre">te.compute(output_shape,</span> <span class="pre">fcompute)</span></code> 这样的接口。fcompute
函数描述了我们要如何计算给定索引的每个元素 <code class="docutils literal notranslate"><span class="pre">[i,</span> <span class="pre">j]</span></code> 的值。</p>
<p><code class="docutils literal notranslate"><span class="pre">te_matmul</span></code> 函数接受一个 <code class="docutils literal notranslate"><span class="pre">te.Tensor</span></code>
类型的对象，并返回矩阵乘法结果。请注意我们是如何根据 A 和 B 的输入 shape
构造计算的。<code class="docutils literal notranslate"><span class="pre">te_matmul</span></code> 适用于具有不同输入 shape 的 A 和 B。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">te_matmul</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">te</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;matmul&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>我们可以使用 A 和 B 获得调用 <code class="docutils literal notranslate"><span class="pre">te_matmul</span></code> 的结果。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">te_matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
<p>要创建 TensorIR 函数，我们可以调用 <code class="docutils literal notranslate"><span class="pre">te.create_prim_func</span></code>
并传入输入和输出值。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>我们可以用类似的方式为 ReLU
计算创建张量表达式。在这里，我们写一个可以适用于具有任何维度数量和 shape
的 <code class="docutils literal notranslate"><span class="pre">te.Tensor</span></code> 的 <code class="docutils literal notranslate"><span class="pre">te_relu</span></code> 函数。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">te_relu</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">te</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">A</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>让我们在两种不同的输入维度数量和 shape 上尝试 <code class="docutils literal notranslate"><span class="pre">te_relu</span></code>。 第一个
<code class="docutils literal notranslate"><span class="pre">X1</span></code> 的尺寸为 <code class="docutils literal notranslate"><span class="pre">(10,)</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X1</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">Y1</span> <span class="o">=</span> <span class="n">te_relu</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">Y1</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>然后是形状为 <code class="docutils literal notranslate"><span class="pre">(10,</span> <span class="pre">20)</span></code> 的 <code class="docutils literal notranslate"><span class="pre">X2</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X2</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">Y2</span> <span class="o">=</span> <span class="n">te_relu</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">X2</span><span class="p">,</span> <span class="n">Y2</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">te</span></code> API 允许我们做的另一件事是组合操作并创建“融合
(fused)”算子。例如，我们可以将 matmul 的结果再次应用 relu。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">te_matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">te_relu</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
<p>我们可以通过只传递感兴趣的输入和输出值，跳过中间值来创建一个 TensorIR
函数。 这将导致 matmul 的结果被分配为 TensorIR 函数中的临时空间。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>我们还可以将中间结果 C 传递到参数列表中。在这种情况下，TensorIR
函数希望我们也从调用方传入
C。通常我们建议只传入输入和输出，这样我们就可以在里面进行更高级的融合。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="blockbuilder-irmodule">
<h3><span class="section-number">5.3.2. </span>使用 BlockBuilder 构造 IRModule<a class="headerlink" href="#blockbuilder-irmodule" title="Permalink to this heading">¶</a></h3>
<p>到目前为止，我们已经创建了一个 TensorIR 函数。
为了构建端到端的模型执行，我们还需要能够通过计算图连接多个 TensorIR
函数。</p>
<p>让我们首先创建一个 block builder，它可以帮助我们逐步构建一个
<code class="docutils literal notranslate"><span class="pre">relax.Function</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&quot;float32&quot;</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&quot;float32&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>我们通过创建 block builder 和一系列元张量函数来构造 Relax 函数。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bb</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">BlockBuilder</span><span class="p">()</span>

<span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&quot;main&quot;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">te_matmul</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">te_relu</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">])</span>

<span class="n">MyModule</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="n">MyModule</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="blockbuilder-api">
<h3><span class="section-number">5.3.3. </span>深入理解 BlockBuilder API<a class="headerlink" href="#blockbuilder-api" title="Permalink to this heading">¶</a></h3>
<p>现在让我们深入了解 BlockBuilder 的 API。将 BlockBuilder 代码和生成的
IRModule 并排放置会很有帮助。</p>
<div class="figure align-default">
<img alt="../_images/integration_block_builder.png" src="../_images/integration_block_builder.png" />
</div>
<p>BlockBuilder 带有与 Relax 函数中相应的作用域。例如，<code class="docutils literal notranslate"><span class="pre">bb.dataflow()</span></code>
创建一个 dataflow block，其中所有对 BlockBuilder 的调用都处在 dataflow
block 的作用域中。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&quot;main&quot;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
        <span class="c1"># every emit call generates a variable inside a dataflow block.</span>
</pre></div>
</div>
<p>每个中间结果都是一个 <code class="docutils literal notranslate"><span class="pre">relax.Var</span></code>，对应一个存储计算结果的变量。
<code class="docutils literal notranslate"><span class="pre">DataflowVar</span></code> 表示该变量是 dataflow block（和计算图）内的中间步骤。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">isinstance</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">)</span>
</pre></div>
</div>
<p>Relax 函数中的每一行都是由 <code class="docutils literal notranslate"><span class="pre">emit_te</span></code> 调用生成的。 例如，</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="n">te_matmul</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">),</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>是由如下代码所生成。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">te_matmul</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>在幕后，<code class="docutils literal notranslate"><span class="pre">bb.emit_te</span></code> 做了以下事情：</p>
<ul class="simple">
<li><p>为 A 和 B 创建一个输入 <code class="docutils literal notranslate"><span class="pre">te.placeholder</span></code>。</p></li>
<li><p>通过 <code class="docutils literal notranslate"><span class="pre">te_matmul</span></code> 函数运行它们。</p></li>
<li><p>调用 <code class="docutils literal notranslate"><span class="pre">te.create_prim_func</span></code> 来创建一个 TensorIR 函数。</p></li>
<li><p>通过 <code class="docutils literal notranslate"><span class="pre">call_dps_packed</span></code> 生成对函数的调用。</p></li>
</ul>
<p>我们可以发现，上面 BlockBuilder
构造后的结果是一个有两个中间值的计算图，一个节点对应 <code class="docutils literal notranslate"><span class="pre">te_matmul</span></code>
操作，另一个节点对应 <code class="docutils literal notranslate"><span class="pre">te_relu</span></code>。</p>
<p>我们可以通过 <code class="docutils literal notranslate"><span class="pre">bb.emit_output</span></code> 创建每个 dataflow block 的输出变量。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
<p>上面的代码标志着 <code class="docutils literal notranslate"><span class="pre">D</span></code> 是一个可以在 dataflow block 之外引用的变量。</p>
<p>最后，函数输出由 <code class="docutils literal notranslate"><span class="pre">bb.emit_func_output</span></code> 标记。
我们只能在每个函数作用域内调用一次 <code class="docutils literal notranslate"><span class="pre">emit_func_output</span></code>。</p>
<p>值得注意的是，我们可以在输出阶段指定函数的参数列表。
这样做在我们动态收集参数列表的情况下会有帮助。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&quot;main&quot;</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="c1"># specify parameters in the end</span>
    <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">])</span>
</pre></div>
</div>
<p>或者，我们也可以在函数范围的开头指定参数列表。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># specify parameters in the beginning.</span>
<span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&quot;main&quot;</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">]):</span>
    <span class="o">...</span>
    <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="pytorch">
<h2><span class="section-number">5.4. </span>从 PyTorch 导入模型<a class="headerlink" href="#pytorch" title="Permalink to this heading">¶</a></h2>
<p>现在我们已经学习了以编程方式构建 IRModule 的工具。
让我们使用它们将机器学习模型从 PyTorch 导入成为 IRModule。</p>
<p>大多数机器学习框架都带有计算图抽象，其中每个节点对应一个操作，边对应它们之间的依赖关系。
我们将采用 PyTorch 模型，获取 PyTorch 原生格式的计算图，并将其转换为
IRModule。</p>
<p>让我们从在 PyTorch 中定义一个模型开始。 为了使示例保持一致，我们将使用
matmul + ReLU 示例。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="section" id="torchfx-graphmodule">
<h3><span class="section-number">5.4.1. </span>创建 TorchFX GraphModule<a class="headerlink" href="#torchfx-graphmodule" title="Permalink to this heading">¶</a></h3>
<p>我们使用 TorchFX 来表示来自 PyTorch 的模型的计算图。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">fx_module</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">symbolic_trace</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">fx_module</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">fx_module</span></code>
包含一个简单的计算图，可以打印成表格便于查看。我们的目标是将此图转换为
IRModule。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fx_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">print_tabular</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h3><span class="section-number">5.4.2. </span>构造映射函数<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p>让我们定义整体的翻译逻辑。 主要流程如下：</p>
<ul class="simple">
<li><p>创建一个 <code class="docutils literal notranslate"><span class="pre">node_map</span></code>，将 <code class="docutils literal notranslate"><span class="pre">fx.Node</span></code> 映射到相应的
<code class="docutils literal notranslate"><span class="pre">relax.Var</span></code>，该 <code class="docutils literal notranslate"><span class="pre">relax.Var</span></code> 代表 IRModule 中的已翻译节点。</p></li>
<li><p>以拓扑顺序迭代 FX 图中的节点。</p></li>
<li><p>给定映射输入，获取节点的映射输出。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">map_param</span><span class="p">(</span><span class="n">param</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">fetch_attr</span><span class="p">(</span><span class="n">fx_mod</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to fetch an attr&quot;&quot;&quot;</span>
    <span class="n">target_atoms</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">attr_itr</span> <span class="o">=</span> <span class="n">fx_mod</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">atom</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target_atoms</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">attr_itr</span><span class="p">,</span> <span class="n">atom</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Node referenced nonexistant target </span><span class="si">{</span><span class="s1">&#39;.&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">target_atoms</span><span class="p">[:</span><span class="n">i</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">attr_itr</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">attr_itr</span><span class="p">,</span> <span class="n">atom</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attr_itr</span>

<span class="k">def</span> <span class="nf">from_fx</span><span class="p">(</span><span class="n">fx_mod</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">,</span> <span class="n">call_function_map</span><span class="p">,</span> <span class="n">call_module_map</span><span class="p">):</span>
    <span class="n">input_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">node_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">named_modules</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">fx_mod</span><span class="o">.</span><span class="n">named_modules</span><span class="p">())</span>

    <span class="n">bb</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">BlockBuilder</span><span class="p">()</span>

    <span class="n">fn_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">fn_output</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&quot;main&quot;</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">fx_mod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
                    <span class="c1"># create input placeholder</span>
                    <span class="n">shape</span> <span class="o">=</span> <span class="n">input_shapes</span><span class="p">[</span><span class="n">input_index</span><span class="p">]</span>
                    <span class="n">input_index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">input_var</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span>
                        <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;float32&quot;</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">fn_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
                    <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_var</span>
                <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;get_attr&quot;</span><span class="p">:</span>
                    <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">map_param</span><span class="p">(</span><span class="n">fetch_attr</span><span class="p">(</span><span class="n">fx_mod</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span><span class="p">:</span>
                    <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_function_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">](</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
                    <span class="n">named_module</span> <span class="o">=</span> <span class="n">named_modules</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>
                    <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_module_map</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">named_module</span><span class="p">)](</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">named_module</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
                    <span class="k">assert</span> <span class="n">fn_output</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="n">fn_output</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="c1"># output and finalize the function</span>
        <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">fn_inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</pre></div>
</div>
<p>我们没有在 <code class="docutils literal notranslate"><span class="pre">from_fx</span></code> 函数中定义函数映射。 我们将通过映射提供每个 torch
function 的翻译规则。 具体来说，以下代码块显示了我们如何通过 <code class="docutils literal notranslate"><span class="pre">emit_te</span></code>
API 做到这一点。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">map_matmul</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">te_matmul</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">map_relu</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">te_relu</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>

<span class="n">MyModule</span> <span class="o">=</span> <span class="n">from_fx</span><span class="p">(</span>
    <span class="n">fx_module</span><span class="p">,</span>
    <span class="n">input_shapes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">)],</span>
    <span class="n">call_function_map</span> <span class="o">=</span> <span class="p">{</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">:</span> <span class="n">map_matmul</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">:</span> <span class="n">map_relu</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">call_module_map</span><span class="o">=</span><span class="p">{},</span>
<span class="p">)</span>

<span class="n">MyModule</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="fashionmnist">
<h2><span class="section-number">5.5. </span>回到 FashionMNIST 的例子<a class="headerlink" href="#fashionmnist" title="Permalink to this heading">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;T-shirt/top&#39;</span><span class="p">,</span> <span class="s1">&#39;Trouser&#39;</span><span class="p">,</span> <span class="s1">&#39;Pullover&#39;</span><span class="p">,</span> <span class="s1">&#39;Dress&#39;</span><span class="p">,</span> <span class="s1">&#39;Coat&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Sandal&#39;</span><span class="p">,</span> <span class="s1">&#39;Shirt&#39;</span><span class="p">,</span> <span class="s1">&#39;Sneaker&#39;</span><span class="p">,</span> <span class="s1">&#39;Bag&#39;</span><span class="p">,</span> <span class="s1">&#39;Ankle boot&#39;</span><span class="p">]</span>

<span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Class:&quot;</span><span class="p">,</span> <span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># Hide outputs
!wget -nc https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/e2e_fashionmnist_mlp_model.png" src="../_images/e2e_fashionmnist_mlp_model.png" />
</div>
<p>以上是我们关心的模型，我们可以按如下的方式构建其 PyTorch 模型。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span> <span class="k">as</span> <span class="nn">pkl</span>

<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>

<span class="n">mlp_params</span> <span class="o">=</span> <span class="n">pkl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&quot;fasionmnist_mlp_params.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">linear0</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&quot;w0&quot;</span><span class="p">])</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">linear0</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&quot;b0&quot;</span><span class="p">])</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&quot;w1&quot;</span><span class="p">])</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch_res</span> <span class="o">=</span> <span class="n">mlp_model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)))</span>

<span class="n">pred_kind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">torch_res</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torch Prediction:&quot;</span><span class="p">,</span> <span class="n">class_names</span><span class="p">[</span><span class="n">pred_kind</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<p>让我们尝试通过为相应的 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 定义映射函数来从 FX 转换。
在这里，我们重用了来自 TVM TOPI (TVM operator inventory) 的预定义 TE
库，而不是定义我们自己的张量表达式。</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">topi.nn.dense(x,</span> <span class="pre">w)</span></code> 执行转置矩阵乘法<code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">&#64;</span> <span class="pre">w.T</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">topi.add</span></code> 执行广播加法。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">topi</span>


<span class="k">def</span> <span class="nf">map_nn_linear</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">nn_mod</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">map_param</span><span class="p">(</span><span class="n">nn_mod</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">nn_mod</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">map_param</span><span class="p">(</span><span class="n">nn_mod</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dense</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">map_nn_relu</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">nn_mod</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">map_relu</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">)</span>


<span class="n">MLPModule</span> <span class="o">=</span> <span class="n">from_fx</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">.</span><span class="n">symbolic_trace</span><span class="p">(</span><span class="n">mlp_model</span><span class="p">),</span>
    <span class="n">input_shapes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)],</span>
    <span class="n">call_function_map</span><span class="o">=</span><span class="p">{</span>
    <span class="p">},</span>
    <span class="n">call_module_map</span><span class="o">=</span><span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span> <span class="n">map_nn_linear</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">:</span> <span class="n">map_nn_relu</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="n">MLPModule</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ex</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">MLPModule</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;llvm&quot;</span><span class="p">)</span>
<span class="n">vm</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
<span class="n">data_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>

<span class="n">nd_res</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">](</span><span class="n">data_nd</span><span class="p">)</span>

<span class="n">pred_kind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">nd_res</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLPModule Prediction:&quot;</span><span class="p">,</span> <span class="n">class_names</span><span class="p">[</span><span class="n">pred_kind</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">5.6. </span>备注：翻译成高层算子<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h2>
<p>在大多数机器学习框架中，有时先转换为更高一级的内置的原始算子会更有帮助。下面的代码块给出了一个例子来做到这一点。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">map_nn_relu_op</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">nn_mod</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">map_nn_linear_op</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">node_map</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">nn_mod</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">map_param</span><span class="p">(</span><span class="n">nn_mod</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">map_param</span><span class="p">(</span><span class="n">nn_mod</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

<span class="n">MLPModuleHighLevel</span> <span class="o">=</span> <span class="n">from_fx</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">.</span><span class="n">symbolic_trace</span><span class="p">(</span><span class="n">mlp_model</span><span class="p">),</span>
    <span class="n">input_shapes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)],</span>
    <span class="n">call_function_map</span><span class="o">=</span><span class="p">{</span>
    <span class="p">},</span>
    <span class="n">call_module_map</span><span class="o">=</span><span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span> <span class="n">map_nn_linear_op</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">:</span> <span class="n">map_nn_relu_op</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="n">MLPModuleHighLevel</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>上面展示了我们使用哪些内置的算子将模型导入为 IRModule
后的结果。这些内置算子是 <strong>比 TensorIR
函数更高级别的抽象</strong>。我们可以有不同的机会将这些原始算子进一步转换为库函数或
TensorIR 函数。</p>
<p>在大多数情况下，在有高级算子支持的情况下，转换为高级内置函数会很有帮助。但是，有很多情况下我们找不到对应的高级内置算子或者想直接指定
TensorIR 函数。 在这些情况下，我们可以自定义翻译逻辑或变换从而生成
<code class="docutils literal notranslate"><span class="pre">call_dps_packed</span></code> 或调用库函数。 通常，我们可以结合高级操作、TensorIR
和库抽象来获得最佳结果。 我们将在后续章节中讨论权衡取舍。</p>
</div>
<div class="section" id="id6">
<h2><span class="section-number">5.7. </span>讨论<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h2>
<p>在本章中，我们重点关注了 MLC 流程的 <strong>开发</strong> 部分。
我们研究了从机器学习框架中获取模型到 IRModule 的不同方法。
我们还简要介绍了高级原始运算符。</p>
<p>一旦我们将模型放入 IRModule
中，我们就可以在原始函数和计算图函数上引入更多种类的变换。一个好的 MLC
流程将这些转换组合在一起，形成最终部署形式。</p>
<div class="figure align-default">
<img alt="../_images/mlc_process.png" src="../_images/mlc_process.png" />
</div>
</div>
<div class="section" id="id7">
<h2><span class="section-number">5.8. </span>总结<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>张量表达式 API 允许我们创建原始的 TensorIR 函数。</p></li>
<li><p>BlockBuilder API 通过 <code class="docutils literal notranslate"><span class="pre">emit_te</span></code> 和其他函数创建 IRModule。</p></li>
<li><p>通过将模型转换为 IRModule，实现与现有的机器学习框架的整合。</p></li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">5. 与机器学习框架的整合</a><ul>
<li><a class="reference internal" href="#id2">5.1. 前言</a></li>
<li><a class="reference internal" href="#id3">5.2. 准备工作</a></li>
<li><a class="reference internal" href="#builder-irmodule">5.3. 通过 Builder 构造 IRModule</a><ul>
<li><a class="reference internal" href="#tensorir">5.3.1. 从张量表达式构造 TensorIR</a></li>
<li><a class="reference internal" href="#blockbuilder-irmodule">5.3.2. 使用 BlockBuilder 构造 IRModule</a></li>
<li><a class="reference internal" href="#blockbuilder-api">5.3.3. 深入理解 BlockBuilder API</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch">5.4. 从 PyTorch 导入模型</a><ul>
<li><a class="reference internal" href="#torchfx-graphmodule">5.4.1. 创建 TorchFX GraphModule</a></li>
<li><a class="reference internal" href="#id4">5.4.2. 构造映射函数</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fashionmnist">5.5. 回到 FashionMNIST 的例子</a></li>
<li><a class="reference internal" href="#id5">5.6. 备注：翻译成高层算子</a></li>
<li><a class="reference internal" href="#id6">5.7. 讨论</a></li>
<li><a class="reference internal" href="#id7">5.8. 总结</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../chapter_auto_program_optimization/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>4. 自动程序优化</div>
         </div>
     </a>
     <a id="button-next" href="../chapter_gpu_acceleration/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>6. GPU 硬件加速</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>